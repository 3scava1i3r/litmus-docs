<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Troubleshooting OpenEBS · </title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta property="og:title" content="Troubleshooting OpenEBS · "/><meta property="og:type" content="website"/><meta property="og:url" content="https://docs.openebs.io/index.html"/><meta property="og:description" content="------"/><meta name="twitter:card" content="summary"/><link rel="shortcut icon" href="/img/favicon.ico"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="stylesheet" href="/css/code-blocks-buttons.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/js/code-blocks-buttons.js"></script><link rel="stylesheet" href="/css/main.css"/></head><body class="sideNavVisible doc separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img id="logo" src="/img/OpenEBS-logo.svg" alt=""/><img class="logo-text" src="/img/openebs-docs-logo.svg" alt=""/><h2 class="headerTitle"></h2></a><div class="dropdown"><span class="ver-link">0.8.1</span><span class="ver-link-span"><i class="arrow down"></i></span><div class="dropdown-menu" id="versions-list" aria-labelledby="dropdownMenuButton"><a class="dropdown-item" id="0" href="https://docs.openebs.io">0.8.1</a><a class="dropdown-item" id="1" href="https://v08-docs.openebs.io">0.8.0</a><a class="dropdown-item" id="2" href="https://v07-docs.openebs.io">0.7.0</a><a class="dropdown-item" id="3" href="https://v06-docs.openebs.io">0.6.0</a><a class="dropdown-item" id="4" href="https://v05-docs.openebs.io">0.5.0</a></div></div><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><a href="https://app.mayaonline.io" target="_blank" rel="noreferrer noopener"><div class="float rotate"><img src="../../../../../../docs/assets/mo-logo.png" alt="MayaOnline" height="45px" width="45px" title="Connect to MayaOnline"/></div></a><div class="container docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><i></i></div><h2><i>›</i><span>Advanced Topics</span></h2></div><div class="navGroups"><div class="navGroup navGroupActive"><h3 id="Introduction" class="languages-menu">Introduction</h3><div class="hide" id="Introduction dropdown-content-div"><ul id="languages-dropdown-items"><li class="navListItem"><a class="navItem" href="/docs/next/overview.html">Overview</a></li><li class="navListItem"><a class="navItem" href="/docs/next/features.html">Features and Benefits</a></li><li class="navListItem"><a class="navItem" href="/docs/next/usecases.html">Use cases</a></li><li class="navListItem"><a class="navItem" href="/docs/next/releases.html">Releases</a></li><li class="navListItem"><a class="navItem" href="/docs/next/support.html">Support</a></li><li class="navListItem"><a class="navItem" href="/docs/next/mayaonline.html">MayaOnline</a></li></ul></div></div><div class="navGroup navGroupActive"><h3 id="Concepts" class="languages-menu">Concepts</h3><div class="hide" id="Concepts dropdown-content-div"><ul id="languages-dropdown-items"><li class="navListItem"><a class="navItem" href="/docs/next/cas.html">Container Attached Storage</a></li><li class="navListItem"><a class="navItem" href="/docs/next/architecture.html">OpenEBS Architecture</a></li><li class="navListItem"><a class="navItem" href="/docs/next/casengines.html">CAS Engines (cStor &amp; Jiva)</a></li><li class="navListItem"><a class="navItem" href="/docs/next/cstor.html">cStor</a></li><li class="navListItem"><a class="navItem" href="/docs/next/jiva.html">Jiva</a></li></ul></div></div><div class="navGroup navGroupActive"><h3 id="User Guides" class="languages-menu">User Guides</h3><div class="hide" id="User Guides dropdown-content-div"><ul id="languages-dropdown-items"><li class="navListItem"><a class="navItem" href="/docs/next/quickstart.html">Quickstart</a></li><li class="navListItem"><a class="navItem" href="/docs/next/prerequisites.html">Prerequisites</a></li><li class="navListItem"><a class="navItem" href="/docs/next/installation.html">Installation</a></li><li class="navListItem"><a class="navItem" href="/docs/next/configurepools.html">Configure StoragePools</a></li><li class="navListItem"><a class="navItem" href="/docs/next/configuresc.html">Configuring StorageClasses</a></li><li class="navListItem"><a class="navItem" href="/docs/next/provisionvols.html">Provisioning Volumes</a></li><li class="navListItem"><a class="navItem" href="/docs/next/operations.html">Day 2 Operations</a></li><li class="navListItem"><a class="navItem" href="/docs/next/jivaguide.html">Jiva user guide</a></li><li class="navListItem"><a class="navItem" href="/docs/next/mayactl.html">Mayactl</a></li><li class="navListItem"><a class="navItem" href="/docs/next/backup.html">Backup and Restore</a></li><li class="navListItem"><a class="navItem" href="/docs/next/upgrade.html">Upgrade</a></li><li class="navListItem"><a class="navItem" href="/docs/next/uninstall.html">Uninstall</a></li></ul></div></div><div class="navGroup navGroupActive"><h3 id="Stateful Applications" class="languages-menu">Stateful Applications</h3><div class="hide" id="Stateful Applications dropdown-content-div"><ul id="languages-dropdown-items"><li class="navListItem"><a class="navItem" href="/docs/next/mysql.html">RDS like MySQL</a></li><li class="navListItem"><a class="navItem" href="/docs/next/prometheus.html">Prometheus</a></li><li class="navListItem"><a class="navItem" href="/docs/next/minio.html">Minio</a></li><li class="navListItem"><a class="navItem" href="/docs/next/gitlab.html">GitLab</a></li><li class="navListItem"><a class="navItem" href="/docs/next/percona.html">Percona</a></li><li class="navListItem"><a class="navItem" href="/docs/next/elasticsearch.html">ElasticSearch</a></li><li class="navListItem"><a class="navItem" href="/docs/next/cassandra.html">Cassandra</a></li><li class="navListItem"><a class="navItem" href="/docs/next/nuodb.html">NuoDB</a></li><li class="navListItem"><a class="navItem" href="/docs/next/postgres.html">PostgreSQL</a></li><li class="navListItem"><a class="navItem" href="/docs/next/redis.html">Redis</a></li><li class="navListItem"><a class="navItem" href="/docs/next/mongo.html">MongoDB</a></li><li class="navListItem"><a class="navItem" href="/docs/next/rwm.html">Read-Write-Many (RWM)</a></li></ul></div></div><div class="navGroup navGroupActive"><h3 id="Advanced Topics" class="languages-menu">Advanced Topics</h3><div class="hide" id="Advanced Topics dropdown-content-div"><ul id="languages-dropdown-items"><li class="navListItem"><a class="navItem" href="/docs/next/performance.html">Performance testing</a></li><li class="navListItem"><a class="navItem" href="/docs/next/faq.html">FAQs</a></li><li class="navListItem"><a class="navItem" href="/docs/next/k8supgrades.html">Kubernetes upgrades</a></li><li class="navListItem navListItemActive"><a class="navItem navItemActive" href="/docs/next/troubleshooting.html">Troubleshooting</a></li></ul></div></div></div></section></div><script>
          var toggler = document.getElementById('navToggler');
          var nav = document.getElementById('docsNav');
          toggler.onclick = function() {
            nav.classList.toggle('docsSliderActive');
          };
        </script><script>
            var navGroupElem = document.getElementsByClassName("hide");
            for(var k=0; k<navGroupElem.length; k++){
              if (navGroupElem[k].innerHTML.indexOf("navItem navItemActive") != -1) {
                document.getElementById(navGroupElem[k].id).className = "visible";
              }
            }
            var elem = document.getElementsByClassName("languages-menu");
            for (var i=0; i < elem.length; i++){
              document.getElementById(String(elem[i].id)).addEventListener("click", function() {
                sessionStorage.setItem("name", document.getElementById(this.id+" dropdown-content-div").className);
                sessionStorage.setItem("Id", document.getElementById(this.id).id);
                if ( document.getElementById(this.id+" dropdown-content-div").className == "hide") {
                  document.getElementById(this.id+" dropdown-content-div").className = "visible";
                }
                else {
                  document.getElementById(this.id+" dropdown-content-div").className = "hide";
                }
            })};</script><script>
          var SelectedItemClassName = sessionStorage.getItem("name");
          var SelectedItemId = sessionStorage.getItem("Id");
          if (SelectedItemClassName == "hide"){
            document.getElementById( SelectedItemId +" dropdown-content-div").className = "visible";
          }
        </script></nav></div><div class="container mainContainer"><div class="wrapper"><div class="post"><header class="postHeader"><a class="edit-page-link button" href="https://github.com/openebs/openebs-docs/edit/staging/docs/troubleshooting.md" target="_blank" rel="noreferrer noopener">Edit</a><a class="edit-page-link button" href="https://github.com/openebs/openebs/issues/new/" target="_blank" rel="noreferrer noopener">Create An Issue</a><h1>Troubleshooting OpenEBS</h1></header><article><div><span><hr>
<p><font size="5">General guidelines for troubleshooting</font></p>
<p>Connecting Kubernetes cluster to MayaOnline is the simplest and easiest way to monitor OpenEBS resources and volumes. Logs of OpenEBS pods available at MayaOnline are helpful for troubleshooting. Topology views of OpenEBS custom resources provide the live status which are helpful in the troubleshooting process.</p>
<p><strong>Steps for troubleshooting:</strong></p>
<ul>
<li>Join <a href="https://slack.openebs.io" target="_blank">OpenEBS slack </a>community</li>
<li>Connect Kubernetes cluster to MayaOnline and observe the following
<ul>
<li>Any alerts that may be relevant to the issue under troubleshooting</li>
<li>Logs that throw up any errors</li>
<li>Status of custom resources of OpenEBS volumes in the topology view</li>
</ul></li>
<li>Search for any reported issues on <a href=" https://stackoverflow.com/questions/tagged/openebs" target="_blank">StackOverflow under OpenEBS tag</a></li>
</ul>
<p><br></p>
<hr>
<p><font size="6">Areas of troubleshooting</font></p>
<p><br></p>
<h2><a class="anchor" aria-hidden="true" id="installation"></a><a href="#installation" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Installation</h2>
<p><a href="#install-failed-user-rights">Installation failed because insufficient user rights</a></p>
<p><a href="#install-failed-iscsi-not-configured">iSCSI client is not setup on Nodes. Application Pod is in ContainerCreating state.</a></p>
<p><a href="#openebs-provsioner-restart-continuously">Why does OpenEBS provisioner pod restart continuously?</a></p>
<p><a href="#install-failed-azure-no-rbac-set">OpenEBS installation fails on Azure</a>.</p>
<p><a href="#multipath-conf-claims-all-scsi-devices-openshift">A multipath.conf file claims all SCSI devices in OpenShift</a>
<br></p>
<h2><a class="anchor" aria-hidden="true" id="uninstall"></a><a href="#uninstall" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Uninstall</h2>
<p><a href="#cvr-deletion-unsuccessful">cStor Volume Replicas are not getting deleted properly</a></p>
<p><a href="#jiva-deletion-scrub-job">Whenever a Jiva PVC is deleted, a job will created and status is seeing as <code>completed</code></a></p>
<h2><a class="anchor" aria-hidden="true" id="volume-provisioning"></a><a href="#volume-provisioning" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Volume provisioning</h2>
<p><a href="#application-read-only">Application complaining ReadOnly filesystem</a></p>
<p><a href="#application-pod-not-running-Rancher">Application pods are not running when OpenEBS volumes are provisioned on Rancher</a></p>
<p><a href="#application-pod-stuck-after-deployment">Application pod is stuck in ContainerCreating state after deployment</a></p>
<p><a href="#cstor-pool-failed-centos-partion-disk">Creating cStor pool fails on CentOS when there are partitions on the disk</a></p>
<p><a href="#application-crashloopbackoff">Application pod enters CrashLoopBackOff state</a></p>
<p><a href="#cstor-pool-pod-not-running">cStor pool pods are not running</a></p>
<p><a href="#Jiva-provisioning-failed-080">OpenEBS Jiva PVC is not provisioning in 0.8.0</a></p>
<p><a href="#recovery-readonly-when-kubelet-is-container">Recovery procedure for Read-only volume where kubelet is running in a container</a></p>
<p><a href="#recovery-readonly-xfs-volume">Recovery procedure for Read-only volume for XFS formatted volumes</a></p>
<p><a href="#unable-to-clone-from-snapshot">Unable to clone OpenEBS volume from snapshot</a></p>
<p><br></p>
<h2><a class="anchor" aria-hidden="true" id="kubernetes-related"></a><a href="#kubernetes-related" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Kubernetes related</h2>
<p><a href="#node-reboot-when-kubelet-memory-increases">Kubernetes node reboots because of increase in memory consumed by Kubelet</a></p>
<p><a href="#Pods-restart-terminate-when-heavy-load">Application and OpenEBS pods terminate/restart under heavy I/O load</a></p>
<p><br></p>
<hr>
<p><br></p>
<p><font size="6" color="blue">Installation</font></p>
<p><h3><a class="anchor" aria-hidden="true" id="install-failed-user-rights"></a>Installation failed because of insufficient user rights</h3></p>
<p>OpenEBS installation can fail in some cloud platform with the following errors.</p>
<pre><code class="hljs">namespace <span class="hljs-string">"openebs"</span> created
serviceaccount <span class="hljs-string">"openebs-maya-operator"</span> created
clusterrolebinding.rbac.authorization.k8s.io <span class="hljs-string">"openebs-maya-operator"</span> created
deployment.apps <span class="hljs-string">"maya-apiserver"</span> created
service <span class="hljs-string">"maya-apiserver-service"</span> created
deployment.apps <span class="hljs-string">"openebs-provisioner"</span> created
deployment.apps <span class="hljs-string">"openebs-snapshot-operator"</span> created
configmap <span class="hljs-string">"openebs-ndm-config"</span> created
daemonset.extensions <span class="hljs-string">"openebs-ndm"</span> created
Error from server (Forbidden): error when creating <span class="hljs-string">"https://raw.githubusercontent.com/openebs/openebs/v0.8.x/k8s/openebs-operator.yaml"</span>: clusterroles.rbac.authorization.k8s.io <span class="hljs-string">"openebs-maya-operator"</span> is forbidden: attempt to grant extra privileges: <span class="hljs-string">[{[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[nodes]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[nodes/proxy]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[namespaces]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[services]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[pods]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[deployments]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[events]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[endpoints]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[configmaps]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[jobs]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[storageclasses]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[persistentvolumeclaims]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[persistentvolumes]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[get]</span> <span class="hljs-string">[volumesnapshot.external-storage.k8s.io]</span> <span class="hljs-string">[volumesnapshots]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[list]</span> <span class="hljs-string">[volumesnapshot.external-storage.k8s.io]</span> <span class="hljs-string">[volumesnapshots]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[watch]</span> <span class="hljs-string">[volumesnapshot.external-storage.k8s.io]</span> <span class="hljs-string">[volumesnapshots]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[create]</span> <span class="hljs-string">[volumesnapshot.external-storage.k8s.io]</span> <span class="hljs-string">[volumesnapshots]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[update]</span> <span class="hljs-string">[volumesnapshot.external-storage.k8s.io]</span> <span class="hljs-string">[volumesnapshots]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[patch]</span> <span class="hljs-string">[volumesnapshot.external-storage.k8s.io]</span> <span class="hljs-string">[volumesnapshots]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[delete]</span> <span class="hljs-string">[volumesnapshot.external-storage.k8s.io]</span> <span class="hljs-string">[volumesnapshots]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[get]</span> <span class="hljs-string">[volumesnapshot.external-storage.k8s.io]</span> <span class="hljs-string">[volumesnapshotdatas]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[list]</span> <span class="hljs-string">[volumesnapshot.external-storage.k8s.io]</span> <span class="hljs-string">[volumesnapshotdatas]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[watch]</span> <span class="hljs-string">[volumesnapshot.external-storage.k8s.io]</span> <span class="hljs-string">[volumesnapshotdatas]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[create]</span> <span class="hljs-string">[volumesnapshot.external-storage.k8s.io]</span> <span class="hljs-string">[volumesnapshotdatas]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[update]</span> <span class="hljs-string">[volumesnapshot.external-storage.k8s.io]</span> <span class="hljs-string">[volumesnapshotdatas]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[patch]</span> <span class="hljs-string">[volumesnapshot.external-storage.k8s.io]</span> <span class="hljs-string">[volumesnapshotdatas]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[delete]</span> <span class="hljs-string">[volumesnapshot.external-storage.k8s.io]</span> <span class="hljs-string">[volumesnapshotdatas]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[get]</span> <span class="hljs-string">[apiextensions.k8s.io]</span> <span class="hljs-string">[customresourcedefinitions]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[list]</span> <span class="hljs-string">[apiextensions.k8s.io]</span> <span class="hljs-string">[customresourcedefinitions]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[create]</span> <span class="hljs-string">[apiextensions.k8s.io]</span> <span class="hljs-string">[customresourcedefinitions]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[update]</span> <span class="hljs-string">[apiextensions.k8s.io]</span> <span class="hljs-string">[customresourcedefinitions]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[delete]</span> <span class="hljs-string">[apiextensions.k8s.io]</span> <span class="hljs-string">[customresourcedefinitions]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[disks]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[storagepoolclaims]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[storagepools]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[castemplates]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[runtasks]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[cstorpools]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[cstorvolumereplicas]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[*]</span> <span class="hljs-string">[*]</span> <span class="hljs-string">[cstorvolumes]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[get]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[/metrics]</span>}] user=&amp;{user.name@mayadata.io  <span class="hljs-string">[system:authenticated]</span> map<span class="hljs-string">[user-assertion.cloud.google.com:[AKUJVpmzjjLCED3Vk2Q7wSjXV1gJs/pA3V9ZW53TOjO5bHOExEps6b2IZRjnru9YBKvaj3pgVu+34A0fKIlmLXLHOQdL/uFA4WbKbKfMdi1XC52CcL8gGTXn0/G509L844+OiM+mDJUftls7uIgOIRFAyk2QBixnYv22ybLtO2n8kcpou+ZcNFEVAD6z8Xy3ZLEp9pMd9WdQuttS506x5HIQSpDggWFf9T96yPc0CYmVEmkJm+O7uw==]</span>]} ownerrules=<span class="hljs-string">[{[create]</span> <span class="hljs-string">[authorization.k8s.io]</span> <span class="hljs-string">[selfsubjectaccessreviews selfsubjectrulesreviews]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span>} {<span class="hljs-string">[get]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[]</span> <span class="hljs-string">[/api /api/* /apis /apis/* /healthz /openapi /openapi/* /swagger-2.0.0.pb-v1 /swagger.json /swaggerapi /swaggerapi/* /version /version/]</span>}] ruleResolutionErrors=<span class="hljs-string">[]</span>
</code></pre>
<p><strong>Troubleshooting</strong></p>
<p>You must enable RBAC before OpenEBS installation. This can be done from the kubernetes master console by executing the following command.</p>
<pre><code class="hljs">kubectl <span class="hljs-keyword">create</span> clusterrolebinding  &lt;cluster_name&gt;-admin-binding --clusterrole=<span class="hljs-keyword">cluster</span>-admin --<span class="hljs-keyword">user</span>=&lt;<span class="hljs-keyword">user</span>-registered-email-<span class="hljs-keyword">with</span>-the-provider&gt;
</code></pre>
<p><h3><a class="anchor" aria-hidden="true" id="install-failed-iscsi-not-configured"></a>iSCSI client is not setup on Nodes. Pod is in ContainerCreating state.</h3></p>
<p>After OpenEBS installation, you may proceed with application deployment which will provision OpenEBS volume. This may fail due to the following error. This can be found by describing the application pod.</p>
<pre><code class="hljs">MountVolume.WaitForAttach failed <span class="hljs-keyword">for</span> volume “pvc-ea5b871b-<span class="hljs-number">32</span>d3-<span class="hljs-number">11</span>e9-<span class="hljs-number">9</span>bf5-<span class="hljs-number">0</span>a8e969eb15a” : <span class="hljs-keyword">open</span> /sys/class/iscsi_hos<span class="hljs-variable">t:</span> <span class="hljs-keyword">no</span> such <span class="hljs-keyword">file</span> <span class="hljs-built_in">or</span> directory -
</code></pre>
<p><strong>Troubleshooting</strong></p>
<p>This logs points that iscsid.service may not be enabled and running on your Nodes. You need to check if the service <code>iscsid.service</code> is running. If it is not running, you have to <code>enable</code> and <code>start</code> the service. You can refer <a href="/docs/next/prerequisites.html">prerequisites</a> section and choose your platform to get the steps for enabling it.</p>
<p><h3><a class="anchor" aria-hidden="true" id="openebs-provsioner-restart-continuously"></a>Why does OpenEBS provisioner pod restart continuously?</h3></p>
<p>The following output displays the pod status of all namespaces in which the OpenEBS provisioner is restarting continuously.</p>
<pre><code class="hljs">NAMESPACE     NAME                                         READY     STATUS             RESTARTS   AGE       IP                NODE
<span class="hljs-section">default</span>       percona                                      <span class="hljs-number">0</span>/<span class="hljs-number">1</span>       Pending            <span class="hljs-number">0</span>          <span class="hljs-number">36</span>m       &lt;none&gt;            &lt;none&gt;
kube-system   calico-etcd-tl4td                            <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running            <span class="hljs-number">0</span>          <span class="hljs-number">1</span>h        <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.65</span>     master
kube-system   calico-kube-controllers<span class="hljs-number">-84</span>fd4db7cd-jz9wt     <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running            <span class="hljs-number">0</span>          <span class="hljs-number">1</span>h        <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.65</span>     master
kube-system   calico-node<span class="hljs-number">-5</span>rqdl                            <span class="hljs-number">2</span>/<span class="hljs-number">2</span>       Running            <span class="hljs-number">0</span>          <span class="hljs-number">1</span>h        <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.65</span>     master
kube-system   calico-node-zt95x                            <span class="hljs-number">2</span>/<span class="hljs-number">2</span>       Running            <span class="hljs-number">0</span>          <span class="hljs-number">1</span>h        <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.66</span>     node
kube-system   coredns<span class="hljs-number">-78</span>fcdf6894<span class="hljs-number">-2</span>plxb                     <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running            <span class="hljs-number">0</span>          <span class="hljs-number">1</span>h        <span class="hljs-number">192.168</span><span class="hljs-number">.219</span><span class="hljs-number">.65</span>    master
kube-system   coredns<span class="hljs-number">-78</span>fcdf6894-gcjj7                     <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running            <span class="hljs-number">0</span>          <span class="hljs-number">1</span>h        <span class="hljs-number">192.168</span><span class="hljs-number">.219</span><span class="hljs-number">.66</span>    master
kube-system   etcd-master                                  <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running            <span class="hljs-number">0</span>          <span class="hljs-number">1</span>h        <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.65</span>     master
kube-system   kube-apiserver-master                        <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running            <span class="hljs-number">0</span>          <span class="hljs-number">1</span>h        <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.65</span>     master
kube-system   kube-controller-manager-master               <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running            <span class="hljs-number">0</span>          <span class="hljs-number">1</span>h        <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.65</span>     master
kube-system   kube-proxy<span class="hljs-number">-9</span>t98s                             <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running            <span class="hljs-number">0</span>          <span class="hljs-number">1</span>h        <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.65</span>     master
kube-system   kube-proxy-mwk9f                             <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running            <span class="hljs-number">0</span>          <span class="hljs-number">1</span>h        <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.66</span>     node
kube-system   kube-scheduler-master                        <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running            <span class="hljs-number">0</span>          <span class="hljs-number">1</span>h        <span class="hljs-number">192.168</span><span class="hljs-number">.56</span><span class="hljs-number">.65</span>     master
openebs       maya-apiserver<span class="hljs-number">-5598</span>cf68ff-tndgm              <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running            <span class="hljs-number">0</span>          <span class="hljs-number">1</span>h        <span class="hljs-number">192.168</span><span class="hljs-number">.167</span><span class="hljs-number">.131</span>   node
openebs       openebs-provisioner<span class="hljs-number">-776846</span>bbff-rqfzr         <span class="hljs-number">0</span>/<span class="hljs-number">1</span>       CrashLoopBackOff   <span class="hljs-number">16</span>         <span class="hljs-number">1</span>h        <span class="hljs-number">192.168</span><span class="hljs-number">.167</span><span class="hljs-number">.129</span>   node
openebs       openebs-snapshot-operator<span class="hljs-number">-5</span>b5f97dd7f-np79k   <span class="hljs-number">0</span>/<span class="hljs-number">2</span>       CrashLoopBackOff   <span class="hljs-number">32</span>         <span class="hljs-number">1</span>h        <span class="hljs-number">192.168</span><span class="hljs-number">.167</span><span class="hljs-number">.130</span>   node
</code></pre>
<p><strong>Troubleshooting</strong></p>
<p>Perform the following steps to verify if the issue is due to misconfiguration while installing the network component.</p>
<ol>
<li>Check if your network related pods are running fine.</li>
<li>Check if OpenEBS provisioner HTTPS requests are reaching the apiserver</li>
<li>Use the latest version of network provider images.</li>
<li>Try other network components such as Calico, kube-router etc. if you are not using any of these.</li>
</ol>
<p><h3><a class="anchor" aria-hidden="true" id="install-failed-azure-no-rbac-set"></a>OpenEBS installation fails on Azure</h3></p>
<p>On AKS, while installing OpenEBS using Helm,  you may see the following error.</p>
<pre><code class="hljs">$ helm installstable/openebs --name openebs --namespace openebs
<span class="hljs-string">Error:</span> release <span class="hljs-string">openebsfailed:</span> clusterroles.rbac.authorization.k8s.io <span class="hljs-string">"openebs"</span> <span class="hljs-string">isforbidden:</span> attempt to grant extra <span class="hljs-string">privileges:</span>[PolicyRule{<span class="hljs-string">Resources:</span>[<span class="hljs-string">"nodes"</span>], <span class="hljs-string">APIGroups:</span>[<span class="hljs-string">"*"</span>],<span class="hljs-string">Verbs:</span>[<span class="hljs-string">"get"</span>]} PolicyRule{<span class="hljs-string">Resources:</span>[<span class="hljs-string">"nodes"</span>],<span class="hljs-string">APIGroups:</span>[<span class="hljs-string">"*"</span>], <span class="hljs-string">Verbs:</span>[<span class="hljs-string">"list"</span>]}PolicyRule{<span class="hljs-string">Resources:</span>[<span class="hljs-string">"nodes"</span>], <span class="hljs-string">APIGroups:</span>[<span class="hljs-string">"*"</span>],<span class="hljs-string">Verbs:</span>[<span class="hljs-string">"watch"</span>]} PolicyRule{<span class="hljs-string">Resources:</span>[<span class="hljs-string">"nodes/proxy"</span>],<span class="hljs-string">APIGroups:</span>[<span class="hljs-string">"*"</span>], <span class="hljs-string">Verbs:</span>[<span class="hljs-string">"get"</span>]}PolicyRule{<span class="hljs-string">Resources:</span>[<span class="hljs-string">"nodes/proxy"</span>], <span class="hljs-string">APIGroups:</span>[<span class="hljs-string">"*"</span>],<span class="hljs-string">Verbs:</span>[<span class="hljs-string">"list"</span>]} PolicyRule{<span class="hljs-string">Resources:</span>[<span class="hljs-string">"nodes/proxy"</span>],<span class="hljs-string">APIGroups:</span>[<span class="hljs-string">"*"</span>], <span class="hljs-string">Verbs:</span>[<span class="hljs-string">"watch"</span>]}PolicyRule{<span class="hljs-string">Resources:</span>[<span class="hljs-string">"namespaces"</span>], <span class="hljs-string">APIGroups:</span>[<span class="hljs-string">"*"</span>],<span class="hljs-string">Verbs:</span>[<span class="hljs-string">"*"</span>]} PolicyRule{<span class="hljs-string">Resources:</span>[<span class="hljs-string">"services"</span>],<span class="hljs-string">APIGroups:</span>[<span class="hljs-string">"*"</span>], <span class="hljs-string">Verbs:</span>[<span class="hljs-string">"*"</span>]} PolicyRule{<span class="hljs-string">Resources:</span>[<span class="hljs-string">"pods"</span>],<span class="hljs-string">APIGroups:</span>[<span class="hljs-string">"*"</span>], <span class="hljs-string">Verbs:</span>[<span class="hljs-string">"*"</span>]}PolicyRule{<span class="hljs-string">Resources:</span>[<span class="hljs-string">"deployments"</span>], <span class="hljs-string">APIGroups:</span>[<span class="hljs-string">"*"</span>],<span class="hljs-string">Verbs:</span>[<span class="hljs-string">"*"</span>]} PolicyRule{<span class="hljs-string">Resources:</span>[<span class="hljs-string">"events"</span>],<span class="hljs-string">APIGroups:</span>[<span class="hljs-string">"*"</span>], <span class="hljs-string">Verbs:</span>[<span class="hljs-string">"*"</span>]}PolicyRule{<span class="hljs-string">Resources:</span>[<span class="hljs-string">"endpoints"</span>], <span class="hljs-string">APIGroups:</span>[<span class="hljs-string">"*"</span>],<span class="hljs-string">Verbs:</span>[<span class="hljs-string">"*"</span>]} PolicyRule{<span class="hljs-string">Resources:</span>[<span class="hljs-string">"persistentvolumes"</span>],<span class="hljs-string">APIGroups:</span>[<span class="hljs-string">"*"</span>], <span class="hljs-string">Verbs:</span>[<span class="hljs-string">"*"</span>]} PolicyRule{<span class="hljs-string">Resources:</span>[<span class="hljs-string">"persistentvolumeclaims"</span>],<span class="hljs-string">APIGroups:</span>[<span class="hljs-string">"*"</span>], <span class="hljs-string">Verbs:</span>[<span class="hljs-string">"*"</span>]}PolicyRule{<span class="hljs-string">Resources:</span>[<span class="hljs-string">"storageclasses"</span>],<span class="hljs-string">APIGroups:</span>[<span class="hljs-string">"storage.k8s.io"</span>], <span class="hljs-string">Verbs:</span>[<span class="hljs-string">"*"</span>]}PolicyRule{<span class="hljs-string">Resources:</span>[<span class="hljs-string">"storagepools"</span>], <span class="hljs-string">APIGroups:</span>[<span class="hljs-string">"*"</span>],<span class="hljs-string">Verbs:</span>[<span class="hljs-string">"get"</span>]} PolicyRule{<span class="hljs-string">Resources:</span>[<span class="hljs-string">"storagepools"</span>], <span class="hljs-string">APIGroups:</span>[<span class="hljs-string">"*"</span>],<span class="hljs-string">Verbs:</span>[<span class="hljs-string">"list"</span>]} PolicyRule{<span class="hljs-string">NonResourceURLs:</span>[<span class="hljs-string">"/metrics"</span>],<span class="hljs-string">Verbs:</span>[<span class="hljs-string">"get"</span>]}] user=&amp;{<span class="hljs-string">system:</span><span class="hljs-string">serviceaccount:</span>kube-<span class="hljs-string">system:</span>tiller6f3172cc<span class="hljs-number">-4</span>a08<span class="hljs-number">-11e8</span><span class="hljs-number">-9</span>af5<span class="hljs-number">-0</span>a58ac1f1729 [<span class="hljs-string">system:</span>serviceaccounts <span class="hljs-string">system:</span><span class="hljs-string">serviceaccounts:</span>kube-<span class="hljs-string">systemsystem:</span>authenticated] map[]} ownerrules=[]ruleResolutionErrors=[clusterroles.rbac.authorization.k8s.io<span class="hljs-string">"cluster-admin"</span> not found]
</code></pre>
<p><strong>Troubleshooting</strong></p>
<p>You must enable RBAC on Azure before OpenEBS installation. For more details, see <a href="/docs/next/prerequisites.html">Prerequisites</a>.</p>
<p><h3><a class="anchor" aria-hidden="true" id="multipath-conf-claims-all-scsi-devices-openshift"></a>A multipath.conf file claims all SCSI devices in OpenShift</h3></p>
<p>A multipath.conf file without either find_multipaths or a manual blacklist claims all SCSI devices.</p>
<p><strong><font size="4">Workaround</font>:</strong></p>
<ol>
<li><p>Add the find_multipaths line to <em>/etc/multipath.conf</em> file similar to the following snippet.</p>
<pre><code class="hljs"><span class="hljs-section">defaults</span> {
    <span class="hljs-attribute">user_friendly_names</span> <span class="hljs-literal">yes</span>
    find_multipaths <span class="hljs-literal">yes</span>
}
</code></pre></li>
<li><p>Run <code>multipath -w /dev/sdc</code> command (replace the devname with your persistent devname).</p></li>
</ol>
<p><br></p>
<hr>
<p><br></p>
<p><font size="6" color="maroon">Un-Install</font></p>
<p><h3><a class="anchor" aria-hidden="true" id="cvr-deletion-unsuccessful"></a>cStor Volume Replicas are not getting deleted properly.</h3></p>
<p>Sometimes, there are chances that cStor volumes may not get deleted. Below workaround will resolve this issue. Perform the following command.</p>
<pre><code class="hljs">kubectl <span class="hljs-builtin-name">edit</span> cvr -n openebs
</code></pre>
<p>And then remove finalizers from the corresponding CVR. Need to remove following entries and save it.</p>
<pre><code class="hljs">finalizers:
- cstorvolumereplica<span class="hljs-selector-class">.openebs</span><span class="hljs-selector-class">.io</span>/finalizer
</code></pre>
<p>This will automatically remove the pending CVR and delete the cStor volume completely.</p>
<p>If there are multiple CVR entries need to be deleted,this can be done by using the following command.</p>
<pre><code class="hljs">CRD=`kubectl <span class="hljs-built_in">get</span> crd | grep cstorvolumereplica | <span class="hljs-built_in">cut</span> -d<span class="hljs-string">" "</span> -f1` &amp;&amp; kubectl patch crd $CRD -p '{<span class="hljs-string">"metadata"</span>:{<span class="hljs-string">"finalizers"</span>: [<span class="hljs-built_in">null</span>]}}' --<span class="hljs-built_in">type</span>=merge
</code></pre>
<p><h3><a class="anchor" aria-hidden="true" id="jiva-deletion-scrub-job"></a>Whenever a Jiva based PVC is deleted, a new job gets created.</h3></p>
<p>As part of deleting the Jiva Volumes, OpenEBS launches scrub jobs for clearing data from the nodes. The completed jobs can be cleared using following command.</p>
<pre><code class="hljs">kubectl delete jobs -l openebs.io/cas-<span class="hljs-class"><span class="hljs-keyword">type</span></span>=jiva -n &lt;<span class="hljs-keyword">namespace</span>&gt;
</code></pre>
<p><br></p>
<hr>
<br>
<p><font size="6" color="red"> Volume provisioning</font></p>
<p><br></p>
<p><h3><a class="anchor" aria-hidden="true" id="application-read-only"></a> Application complaining ReadOnly filesystem</h3></p>
<p>Application sometimes complain about the underlying filesystem has become ReadOnly.</p>
<p><strong>Troubleshooting</strong></p>
<p>This can happen for many reasons.</p>
<ul>
<li>The cStor target pod is evicted because of resource constraints and is not scheduled within time</li>
<li>Node is rebooted in adhoc manner (or unscheduled reboot) and Kubernetes is waiting for Kubelet to come backup to know that the node is rebooted and the pods on that node need to be rescheduled. Kubernetes can take upto 30 minutes as timeout before deciding the node does not comebackup and pods need to be rescheduled. During this time, the iSCSI initiator at the application pod has timeout and marked the underlying filesystem as ReadOnly</li>
<li>cStor target has lost quorum because of underlying node losses and target has marked the lun as ReadOnly</li>
</ul>
<p>Go through the Kubelet logs and application pod logs to know the reason for marking the ReadOnly and take appropriate action. <a href="/docs/next/k8supgrades.html">Maintaining volume quorum</a> is necessary during Kuberntes node reboots.</p>
<p><h3><a class="anchor" aria-hidden="true" id="application-pod-not-running-Rancher"></a>Application pods are not running when OpenEBS volumes are provisioned on Rancher</h3></p>
<p>The setup environment where the issue occurs is rancher/rke with bare metal hosts running CentOS. After installing OpenEBS, OpenEBS pods are running, but application pod is in <em>ContainerCreating</em> state. It consume Jiva volume.The output of <code>kubectl get pods</code> is displayed as follows.</p>
<pre><code class="hljs">NAME                                                             READY     STATUS              RESTARTS   AGE
nginx-deployment<span class="hljs-number">-57849</span>d9f57-gvzkh                                <span class="hljs-number">0</span>/<span class="hljs-number">1</span>       ContainerCreating   <span class="hljs-number">0</span>          <span class="hljs-number">2</span>m
pvc-adb79406<span class="hljs-number">-8e3</span>e<span class="hljs-number">-11e8</span>-a06a<span class="hljs-number">-001</span>c42c2325f-ctrl<span class="hljs-number">-58</span>dcdf997f-n4kd9   <span class="hljs-number">2</span>/<span class="hljs-number">2</span>       Running             <span class="hljs-number">0</span>          <span class="hljs-number">8</span>m
pvc-adb79406<span class="hljs-number">-8e3</span>e<span class="hljs-number">-11e8</span>-a06a<span class="hljs-number">-001</span>c42c2325f-rep<span class="hljs-number">-696</span>b599894-gq4z6    <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running             <span class="hljs-number">0</span>          <span class="hljs-number">8</span>m
pvc-adb79406<span class="hljs-number">-8e3</span>e<span class="hljs-number">-11e8</span>-a06a<span class="hljs-number">-001</span>c42c2325f-rep<span class="hljs-number">-696</span>b599894-hwx52    <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running             <span class="hljs-number">0</span>          <span class="hljs-number">8</span>m
pvc-adb79406<span class="hljs-number">-8e3</span>e<span class="hljs-number">-11e8</span>-a06a<span class="hljs-number">-001</span>c42c2325f-rep<span class="hljs-number">-696</span>b599894-vs97n    <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running             <span class="hljs-number">0</span>          <span class="hljs-number">8</span>m
</code></pre>
<p><strong>Troubleshooting</strong></p>
<p>SCSI package is installed on both Host and RKE kubelet.</p>
<pre><code class="hljs">[root@<span class="hljs-keyword">node</span><span class="hljs-title">-34622</span> ~]<span class="hljs-comment"># iscsiadm -V</span>
iscsiadm <span class="hljs-keyword">version</span> <span class="hljs-number">6.2</span>.<span class="hljs-number">0.874</span>-<span class="hljs-number">7</span>
[root@<span class="hljs-keyword">node</span><span class="hljs-title">-34622</span> ~]<span class="hljs-comment"># docker exec kubelet iscsiadm -V</span>
iscsiadm <span class="hljs-keyword">version</span> <span class="hljs-number">2.0</span>-<span class="hljs-number">874</span>
</code></pre>
<p>If output returns iscsiadm version for both commands, then you have to remove iSCSI from the node. You will find the resolution method from <a href="/docs/next/iscsiclient.html#aks">here</a>.</p>
<p><h3><a class="anchor" aria-hidden="true" id="application-pod-stuck-after-deployment"></a>Application pod is stuck in ContainerCreating state after deployment</h3></p>
<p><strong>Troubleshooting</strong></p>
<ul>
<li><p>Obtain the output of the <code>kubectl describe pod &lt;application_pod&gt;</code> and check the events.</p></li>
<li><p>If the error message <em>executable not found in $PATH</em> is found, check whether the iSCSI initiator utils are installed on the node/kubelet container (rancherOS, coreOS). If not, install the same and retry deployment.</p></li>
<li><p>If the warning message <em>FailedMount: Unable to mount volumes for pod &lt;&gt;: timeout expired waiting for volumes to attach/mount</em> is persisting use the following procedure.</p>
<ol>
<li><p>Check whether the Persistent Volume Claim/Persistent Volume (PVC/PV) are created successfully and the OpenEBS controller and replica pods are running. These can be verified using the <code>kubectl get pvc,pv</code> and <code>kubectl get pods</code>command.</p></li>
<li><p>If the OpenEBS volume pods are not created, and the PVC is in pending state, check whether the storageclass referenced by the application PVC is available/installed. This can be confirmed using the <code>kubectl get sc</code> command. If this storageclass is not created, or improperly created without the appropriate attributes, recreate the same and re-deploy the application.</p>
<p><strong>Note:</strong> Ensure that the older PVC objects are deleted before re-deployment.</p></li>
<li><p>If the PV is created (in bound state), but replicas are not running or are in pending state, perform a <code>kubectl describe &lt;replica_pod&gt;</code> and check the events. If the events indicate <em>FailedScheduling due to Insufficient cpu, NodeUnschedulable or MatchInterPodAffinity and PodToleratesNodeTaints</em>, check the following:</p>
<ul>
<li>replica count is equal to or lesser than available schedulable nodes</li>
<li>there are enough resources on the nodes to run the replica pods</li>
<li>whether nodes are tainted and if so, whether they are tolerated by the OpenEBS replica pods</li>
</ul>
<p>Ensure that the above conditions are met and the replica rollout is successful. This will ensure application enters running state.</p></li>
<li><p>If the PV is created and OpenEBS pods are running, use the <code>iscsiadm -m session</code> command on the node (where the pod is scheduled) to identify whether the OpenEBS iSCSI volume has been attached/logged-into. If not, verify network connectivity between the nodes.</p></li>
<li><p>If the session is present, identify the SCSI device associated with the session using the command <code>iscsiadm -m session -P 3</code>. Once it is confirmed that the iSCSI device is available (check the output of <code>fdisk -l</code> for the mapped SCSI device), check the kubelet and system logs including the iscsid and kernel (syslog) for information on the state of this iSCSI device. If inconsistencies are observed, execute the filesyscheck on the device <code>fsck -y /dev/sd&lt;&gt;</code>. This will mount the volume to the node.</p></li>
</ol></li>
<li><p>In OpenShift deployments, you may face this issue with the OpenEBS replica pods continuously restarting, that is, they are in crashLoopBackOff state. This is due to the default &quot;restricted&quot; security context settings. Edit the following settings using <code>oc edit scc restricted</code> to get the application pod running.</p>
<ul>
<li><em>allowHostDirVolumePlugin: true</em></li>
<li><em>runAsUser: runAsAny</em></li>
</ul></li>
</ul>
<p><h3><a class="anchor" aria-hidden="true" id="cstor-pool-failed-centos-partion-disk"></a>Creating cStor pool fails on CentOS when there are partitions on the disk.</h3></p>
<p>Creating cStor pool fails with the following error message:</p>
<pre><code class="hljs">E0920 <span class="hljs-number">14</span>:<span class="hljs-number">51</span>:<span class="hljs-number">17.474702</span>       <span class="hljs-number">8</span> pool.<span class="hljs-string">go:</span><span class="hljs-number">78</span>] Unable to create <span class="hljs-string">pool:</span> <span class="hljs-regexp">/dev/</span>disk<span class="hljs-regexp">/by-id/</span>ata-WDC_WD2500BPVT<span class="hljs-number">-00</span>JJ
</code></pre>
<p>sdb and sdc are used for cStor pool creation.</p>
<pre><code class="hljs">core@k8worker02 ~ $ lsblk
NAME        MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda           8:0   <span class="hljs-number"> 0 </span>111.8G <span class="hljs-number"> 0 </span>disk
|-sda1        8:1   <span class="hljs-number"> 0 </span>  128M <span class="hljs-number"> 0 </span>part  /boot
|-sda2        8:2   <span class="hljs-number"> 0 </span>    2M <span class="hljs-number"> 0 </span>part
|-sda3        8:3   <span class="hljs-number"> 0 </span>    1G <span class="hljs-number"> 0 </span>part
| `-usr     254:0   <span class="hljs-number"> 0 </span> 1016M <span class="hljs-number"> 1 </span>crypt /usr
|-sda4        8:4   <span class="hljs-number"> 0 </span>    1G <span class="hljs-number"> 0 </span>part
|-sda6        8:6   <span class="hljs-number"> 0 </span>  128M <span class="hljs-number"> 0 </span>part  /usr/share/oem
|-sda7        8:7   <span class="hljs-number"> 0 </span>   64M <span class="hljs-number"> 0 </span>part
`-sda9        8:9   <span class="hljs-number"> 0 </span>109.5G <span class="hljs-number"> 0 </span>part  /
sdb           8:16  <span class="hljs-number"> 0 </span>111.8G <span class="hljs-number"> 0 </span>disk
sdc           8:32  <span class="hljs-number"> 0 </span>232.9G <span class="hljs-number"> 0 </span>disk
|-sdc1        8:33  <span class="hljs-number"> 0 </span>    1G <span class="hljs-number"> 0 </span>part
`-sdc2        8:34  <span class="hljs-number"> 0 </span>231.9G <span class="hljs-number"> 0 </span>part
 |-cl-swap 254:1   <span class="hljs-number"> 0 </span>  7.8G <span class="hljs-number"> 0 </span>lvm
 |-cl-home 254:2   <span class="hljs-number"> 0 </span>174.1G <span class="hljs-number"> 0 </span>lvm
 `-cl-root 254:3   <span class="hljs-number"> 0 </span>   50G <span class="hljs-number"> 0 </span>lvm
</code></pre>
<p><strong>Troubleshooting</strong></p>
<ol>
<li><p>Clear the partitions on the portioned disk.</p></li>
<li><p>Run the following command on the host machine to check any LVM handler on the device.</p>
<pre><code class="hljs"><span class="hljs-attribute">sudo</span> dmsetup <span class="hljs-literal">info</span> -C
</code></pre>
<p>Output of the above command will be similar to the following.</p>
<pre><code class="hljs">Name             Maj Min Stat Open Targ Event  UUID                                                                 
usr              <span class="hljs-number">254</span>   <span class="hljs-number">0</span> L--r    <span class="hljs-number">1</span>    <span class="hljs-number">1</span>      <span class="hljs-number">0</span> CRYPT-VERITY<span class="hljs-number">-959135</span>d6b3894b3b8125503de238d5c4-usr                   
centos-home      <span class="hljs-number">254</span>   <span class="hljs-number">2</span> L--w    <span class="hljs-number">0</span>    <span class="hljs-number">1</span>      <span class="hljs-number">0</span> LVM<span class="hljs-number">-1</span>kqWMeQWqH3qTsiHhYw3ygAzOvpfDL58dDmziWBI0panwOGRq2rp9PjpmE6qdf1V
centos-swap      <span class="hljs-number">254</span>   <span class="hljs-number">1</span> L--w    <span class="hljs-number">0</span>    <span class="hljs-number">1</span>      <span class="hljs-number">0</span> LVM<span class="hljs-number">-1</span>kqWMeQWqH3qTsiHhYw3ygAzOvpfDL58UIVFhLkzvE1mk7uCy2nePlktBHfTuTYF
centos-root      <span class="hljs-number">254</span>   <span class="hljs-number">3</span> L--w    <span class="hljs-number">0</span>    <span class="hljs-number">1</span>      <span class="hljs-number">0</span> LVM<span class="hljs-number">-1</span>kqWMeQWqH3qTsiHhYw3ygAzOvpfDL58WULaIYm0X7QmrwQaWYxz1hTwzWocAwYJ
</code></pre>
<p>If the output is similar to the above, you must remove the handler on the device.</p>
<pre><code class="hljs">sudo dmsetup <span class="hljs-builtin-name">remove</span> centos-home
sudo dmsetup <span class="hljs-builtin-name">remove</span> centos-swap
sudo dmsetup <span class="hljs-builtin-name">remove</span> centos-root
</code></pre></li>
</ol>
<p><h3><a class="anchor" aria-hidden="true" id="application-crashloopbackoff"></a>Application pod enters CrashLoopBackOff states</h3></p>
<p>Application pod enters CrashLoopBackOff state</p>
<p>This issue is due to failed application operations in the container. Typically this is caused due to failed writes on the mounted PV. To confirm this, check the status of the PV mount inside the application pod.</p>
<p><strong>Troubleshooting</strong></p>
<ul>
<li>Perform a <code>kubectl exec -it &lt;app&gt;</code> bash (or any available shell) on the application pod and attempt writes on the volume mount. The volume mount can be obtained either from the application specification (&quot;volumeMounts&quot; in container spec) or by performing a <code>df -h</code> command in the controller shell (the OpenEBS iSCSI device will be mapped to the volume mount).</li>
<li>The writes can be attempted using a simple command like <code>echo abc &gt; t.out</code> on the mount. If the writes fail with <em>Read-only file system errors</em>, it means the iSCSI connections to the OpenEBS volumes are lost. You can confirm by checking the node's system logs including iscsid, kernel (syslog) and the kubectl logs (<code>journalctl -xe, kubelet.log</code>).</li>
<li>iSCSI connections usually fail due to the following.
<ul>
<li>flaky networks (can be confirmed by ping RTTs, packet loss etc.) or failed networks between -
<ul>
<li>OpenEBS PV controller and replica pods</li>
<li>Application and controller pods</li>
</ul></li>
<li>Node failures</li>
<li>OpenEBS volume replica crashes or restarts due to software bugs</li>
</ul></li>
<li>In all the above cases, loss of the device for a period greater than the node iSCSI initiator timeout causes the volumes to be re-mounted as RO.</li>
<li>In certain cases, the node/replica loss can lead to the replica quorum not being met (i.e., less than 51% of replicas available) for an extended period of time, causing the OpenEBS volume to be presented as a RO device.</li>
</ul>
<p><strong>Workaround/Recovery</strong></p>
<p>The procedure to ensure application recovery in the above cases is as follows:</p>
<ol>
<li><p>Resolve the system issues which caused the iSCSI disruption/RO device condition. Depending on the cause, the resolution steps may include recovering the failed nodes, ensuring replicas are brought back on the same nodes as earlier, fixing the network problems and so on.</p></li>
<li><p>Ensure that the OpenEBS volume controller and replica pods are running successfully with all replicas in <em>RW mode</em>. Use the command <code>curl GET http://&lt;ctrl ip&gt;:9501/v1/replicas | grep createTypes</code> to confirm.</p></li>
<li><p>If anyone of the replicas are still in RO mode, wait for the synchronization to complete. If all the replicas are in RO mode (this may occur when all replicas re-register into the controller within short intervals), you must restart the OpenEBS volume controller using the <code>kubectl delete pod &lt;pvc-ctrl&gt;</code> command . Since it is a Kubernetes deployment, the controller pod is restarted successfully. Once done, verify that all replicas transition into <em>RW mode</em>.</p></li>
<li><p>Un-mount the stale iscsi device mounts on the application node. Typically, these devices are mounted in the <code>/var/lib/kubelet/plugins/kubernetes.io/iscsi/iface-default/&lt;target-portal:iqn&gt;-lun-0</code> path.</p>
<p>Example:</p>
<pre><code class="hljs">umount /var/<span class="hljs-class"><span class="hljs-keyword">lib</span>/<span class="hljs-title">kubelet</span>/<span class="hljs-title">plugins</span>/<span class="hljs-title">kubernetes</span>.<span class="hljs-title">io</span>/<span class="hljs-title">iscsi</span>/<span class="hljs-title">iface</span>-<span class="hljs-title">default</span>/10.39.241.26:</span>
<span class="hljs-number">3260</span>-iqn.<span class="hljs-number">2016</span>-<span class="hljs-number">09</span>.com.openebs.<span class="hljs-symbol">jiva:</span>mongo-jiva-mongo-persistent-storage-mongo-<span class="hljs-number">0</span>-<span class="hljs-number">3481266901</span>-lun-<span class="hljs-number">0</span>

umount /var/<span class="hljs-class"><span class="hljs-keyword">lib</span>/<span class="hljs-title">kubelet</span>/<span class="hljs-title">pods</span>/<span class="hljs-title">ae74da97</span>-<span class="hljs-title">c852</span>-11<span class="hljs-title">e8</span>-<span class="hljs-title">a219</span>-42010<span class="hljs-title">af000b6</span>/<span class="hljs-title">volumes</span>/<span class="hljs-title">kuber</span></span>
netes.io~iscsi/mongo-jiva-mongo-persistent-storage-mongo-<span class="hljs-number">0</span>-<span class="hljs-number">3481266901</span>
</code></pre></li>
<li><p>Identify whether the iSCSI session is re-established after failure. This can be verified using <code>iscsiadm -m session</code>, with the device mapping established using <code>iscsiadm -m session -P 3</code> and <code>fdisk -l</code>. <strong>Note:</strong> Sometimes, it is observed that there are stale device nodes (scsi device names) present on the Kubernetes node. Unless the logs confirm that a re-login has occurred after the system issues were resolved, it is recommended to perform the following step after doing a purge/logout of the existing session using <code>iscsiadm -m node -T &lt;iqn&gt; -u</code>.</p></li>
<li><p>If the device is not logged in again, ensure that the network issues/failed nodes/failed replicas are resolved, the device is discovered, and the session is re-established. This can be achieved using the commands <code>iscsiadm -m discovery -t st -p &lt;ctrl svc IP&gt;:3260</code> and <code>iscsiadm -m node -T &lt;iqn&gt; -l</code> respectively.</p></li>
<li><p>Identify the new SCSI device name corresponding to the iSCSI session (the device name may or may not be the same as before).</p></li>
<li><p>Re-mount the new disk into the mountpoint mentioned earlier using the <code>mount -o rw,relatime,data=ordered /dev/sd&lt;&gt; &lt;mountpoint&gt;</code> command. If the re-mount fails due to inconsistencies on the device (unclean filesystem), perform a filesyscheck <code>fsck -y /dev/sd&lt;&gt;</code>.</p></li>
<li><p>Ensure that the application uses the newly mounted disk by forcing it to restart on the same node. Use the command<code>docker stop &lt;id&gt;</code> of the application container on the node. Kubernetes will automatically restart the pod to ensure the &quot;desirable&quot; state.</p>
<p>While this step may not be necessary most times (as the application is already undergoing periodic restarts as part of the CrashLoop cycle), it can be performed if the application pod's next restart is scheduled with an exponential back-off delay.</p></li>
</ol>
<p><strong>Notes:</strong></p>
<ol>
<li>The above procedure works for applications that are either pods or deployments/statefulsets. In case of the latter, the application pod can be restarted (i.e., deleted) after step-4 (iscsi logout) as the deployment/statefulset controller will take care of rescheduling the application on a same/different node with the volume.</li>
</ol>
<p><h3><a class="anchor" aria-hidden="true" id="cstor-pool-pod-not-running"></a>cStor pool pods are not running</h3></p>
<p>The cStor disk pods are not coming up after it deploy with the YAML. On checking the pool pod logs, it says <code>/dev/xvdg is in use and contains a xfs filesystem.</code></p>
<p><strong>Workaround:</strong></p>
<p>cStor can consume disks that are attached (are visible to OS as SCSI devices) to the Nodes and no need of format these disks. This means disks should not have any filesystem and it should be unmounted on the Node. It is optional to wipe out the data from the disk if you use existing disks for cStor pool creation.</p>
<p><h3><a class="anchor" aria-hidden="true" id="Jiva-provisioning-failed-080"></a>OpenEBS Jiva PVC is not provisioning in 0.8.0</h3></p>
<p>Even all OpenEBS pods are in running state, unable to provision Jiva volume if you install through helm.</p>
<p><strong>Troubleshooting:</strong></p>
<p>Check the latest logs showing in the OpenEBS provisioner logs. If the particular PVC creation entry logs are not coming on the OpenEBS provisioner pod, then restart the OpenEBS provisioner pod. From 0.8.1 version, liveness probe feature will check the OpenEBS provisioner pod status periodically and ensure its availability for OpenEBS PVC creation.</p>
<p><h3><a class="anchor" aria-hidden="true" id="recovery-readonly-when-kubelet-is-container"></a>Recovery procedure for Read-only volume where kubelet is running in a container.</h3></p>
<p>In environments where the kubelet runs in a container, perform the following steps as part of the recovery procedure for a Volume-Read only issue.</p>
<ol>
<li>Confirm that the OpenEBS target does not exist as a Read Only device by the OpenEBS controller and that all replicas are in Read/Write mode.
<ul>
<li>Un-mount the iSCSI volume from the node in which the application pod is scheduled.</li>
<li>Perform the following iSCSI operations from inside the kubelet container.
<ul>
<li>Logout</li>
<li>Rediscover</li>
<li>Login</li>
</ul></li>
<li>Perform the following iSCSI operations from inside the kubelet container.</li>
<li>Re-mount the iSCSI device (may appear with a new SCSI device name) on the node.</li>
<li>Verify if the application pod is able to start using/writing into the newly mounted device.</li>
</ul></li>
<li>Once the application is back in &quot;Running&quot; state post recovery by following steps 1-9, if existing/older data is not visible (i.e., it comes up as a fresh instance), it is possible that the application pod is using the docker container filesystem instead of the actual PV (observed sometimes due to the reconciliation attempts by Kubernetes to get the pod to a desired state in the absence of the mounted iSCSI disk). This can be checked by performing a <code>df -h</code> or <code>mount</code> command inside the application pods. These commands should show the scsi device <code>/dev/sd*</code> mounted on the specified mount point. If not, the application pod can be forced to use the PV by restarting it (deployment/statefulset) or performing a docker stop of the application container on the node (pod).</li>
</ol>
<p><h3><a class="anchor" aria-hidden="true" id="recovery-readonly-xfs-volume"></a>Recovery procedure for Read-only volume for XFS formatted volumes</h3></p>
<p>In case of <code>XFS</code> formatted volumes, perform the following steps once the iSCSI target is available in RW state &amp; logged in:</p>
<ul>
<li>Un-mount the iSCSI volume from the node in which the application pod is scheduled. This may cause the application to enter running state by using the local mount point.</li>
<li>Mount to volume to a new (temp) directory to replay the metadata changes in the log</li>
<li>Unmount the volume again</li>
<li>Perform <code>xfs_repair /dev/&lt;device&gt;</code>. This fixes if any file system related errors on the device</li>
<li>Perform application pod deletion to facilitate fresh mount of the volume. At this point, the app pod may be stuck on <code>terminating</code> OR <code>containerCreating</code> state. This can be resolved by deleting the volume folder (w/ app content) on the local directory.</li>
</ul>
<p><h3><a class="anchor" aria-hidden="true" id="unable-to-clone-from-snapshot"></a>Unable to clone OpenEBS volume from snapshot</h3></p>
<p>Taken a snpashot of a PVC successfully. But unable to clone the volume from the snapshot.</p>
<p><strong>Troubleshooting:</strong></p>
<p>Logs from snapshot-controller pods  are follows.</p>
<pre><code class="hljs"><span class="hljs-string">ERROR:</span> logging before flag.<span class="hljs-string">Parse:</span> I0108 <span class="hljs-number">18</span>:<span class="hljs-number">11</span>:<span class="hljs-number">54.017909</span>      <span class="hljs-number">1</span> volume.<span class="hljs-string">go:</span><span class="hljs-number">73</span>] OpenEBS volume provisioner namespace openebs
I0108 <span class="hljs-number">18</span>:<span class="hljs-number">11</span>:<span class="hljs-number">54.181897</span>      <span class="hljs-number">1</span> snapshot-controller.<span class="hljs-string">go:</span><span class="hljs-number">95</span>] starting snapshot controller
I0108 <span class="hljs-number">18</span>:<span class="hljs-number">11</span>:<span class="hljs-number">54.200069</span>      <span class="hljs-number">1</span> snapshot-controller.<span class="hljs-string">go:</span><span class="hljs-number">167</span>] Starting snapshot controller
I0108 <span class="hljs-number">18</span>:<span class="hljs-number">11</span>:<span class="hljs-number">54.200139</span>      <span class="hljs-number">1</span> controller_utils.<span class="hljs-string">go:</span><span class="hljs-number">1027</span>] Waiting <span class="hljs-keyword">for</span> caches to sync <span class="hljs-keyword">for</span> snapshot-controller controller
I0108 <span class="hljs-number">18</span>:<span class="hljs-number">11</span>:<span class="hljs-number">54.300430</span>      <span class="hljs-number">1</span> controller_utils.<span class="hljs-string">go:</span><span class="hljs-number">1034</span>] Caches are synced <span class="hljs-keyword">for</span> snapshot-controller controller
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.170921</span>      <span class="hljs-number">1</span> snapshot-controller.<span class="hljs-string">go:</span><span class="hljs-number">190</span>] [CONTROLLER] OnAdd <span class="hljs-regexp">/apis/</span>volumesnapshot.external-storage.k8s.io<span class="hljs-regexp">/v1/</span>namespaces<span class="hljs-regexp">/default/</span>volumesnapshots<span class="hljs-regexp">/xl-release-snapshot, Snapshot &amp;v1.VolumeSnapshot{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, Metadata:v1.ObjectMeta{Name:"xl-release-snapshot", GenerateName:"", Namespace:"default", SelfLink:"/</span>apis<span class="hljs-regexp">/volumesnapshot.external-storage.k8s.io/</span>v1<span class="hljs-regexp">/namespaces/</span><span class="hljs-keyword">default</span><span class="hljs-regexp">/volumesnapshots/</span>xl-release-snapshot<span class="hljs-string">", UID:"</span>dc804d0d<span class="hljs-number">-139</span>a<span class="hljs-number">-11e9</span><span class="hljs-number">-9561</span><span class="hljs-number">-005056949728</span><span class="hljs-string">", ResourceVersion:"</span><span class="hljs-number">2072353</span><span class="hljs-string">", Generation:1, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63682585945, loc:(*time.Location)(0x2a17900)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string{"</span>kubectl.kubernetes.io<span class="hljs-regexp">/last-applied-configuration":"{\"apiVersion\":\"volumesnapshot.external-storage.k8s.io/</span>v1\<span class="hljs-string">",\"kind\":\"VolumeSnapshot\",\"metadata\":{\"annotations\":{},\"name\":\"xl-release-snapshot\",\"namespace\":\"default\"},\"spec\":{\"persistentVolumeClaimName\":\"xlr-data-pvc\"}}\n"</span>}, <span class="hljs-string">OwnerReferences:</span>[]v1.OwnerReference(nil), <span class="hljs-string">Initializers:</span>(*v1.Initializers)(nil), <span class="hljs-string">Finalizers:</span>[]string(nil), <span class="hljs-string">ClusterName:</span><span class="hljs-string">""</span>}, <span class="hljs-string">Spec:</span>v1.VolumeSnapshotSpec{<span class="hljs-string">PersistentVolumeClaimName:</span><span class="hljs-string">"xlr-data-pvc"</span>, <span class="hljs-string">SnapshotDataName:</span><span class="hljs-string">""</span>}, <span class="hljs-string">Status:</span>v1.VolumeSnapshotStatus{<span class="hljs-string">CreationTimestamp:</span>v1.Time{<span class="hljs-string">Time:</span>time.Time{<span class="hljs-string">wall:</span><span class="hljs-number">0x0</span>, <span class="hljs-string">ext:</span><span class="hljs-number">0</span>, <span class="hljs-string">loc:</span>(*time.Location)(nil)}}, <span class="hljs-string">Conditions:</span>[]v1.VolumeSnapshotCondition(nil)}}
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.210135</span>      <span class="hljs-number">1</span> desired_state_of_world.<span class="hljs-string">go:</span><span class="hljs-number">76</span>] Adding <span class="hljs-keyword">new</span> snapshot to desired state of <span class="hljs-string">world:</span> <span class="hljs-keyword">default</span>/xl-release-snapshot-dc804d0d<span class="hljs-number">-139</span>a<span class="hljs-number">-11e9</span><span class="hljs-number">-9561</span><span class="hljs-number">-005056949728</span>
E0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.288184</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">309</span>] No conditions <span class="hljs-keyword">for</span> <span class="hljs-keyword">this</span> snapshot yet.
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.295175</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">160</span>] No VolumeSnapshotData objects found on the API server
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.295224</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">458</span>] <span class="hljs-string">findSnapshot:</span> snapshot xl-release-snapshot
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.355476</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">469</span>] <span class="hljs-string">findSnapshot:</span> find snapshot xl-release-snapshot by tags &amp;map[].
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.355550</span>      <span class="hljs-number">1</span> processor.<span class="hljs-string">go:</span><span class="hljs-number">183</span>] FindSnapshot by <span class="hljs-string">tags:</span> map[string]string(nil)
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.355575</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">449</span>] <span class="hljs-string">syncSnapshot:</span> Creating snapshot <span class="hljs-keyword">default</span>/xl-release-snapshot-dc804d0d<span class="hljs-number">-139</span>a<span class="hljs-number">-11e9</span><span class="hljs-number">-9561</span><span class="hljs-number">-005056949728</span> ...
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.355603</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">491</span>] <span class="hljs-string">createSnapshot:</span> Creating snapshot <span class="hljs-keyword">default</span>/xl-release-snapshot-dc804d0d<span class="hljs-number">-139</span>a<span class="hljs-number">-11e9</span><span class="hljs-number">-9561</span><span class="hljs-number">-005056949728</span> through the plugin ...
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.373908</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">497</span>] <span class="hljs-string">createSnapshot:</span> Creating metadata <span class="hljs-keyword">for</span> snapshot <span class="hljs-keyword">default</span>/xl-release-snapshot-dc804d0d<span class="hljs-number">-139</span>a<span class="hljs-number">-11e9</span><span class="hljs-number">-9561</span><span class="hljs-number">-005056949728.</span>
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.373997</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">701</span>] In updateVolumeSnapshotMetadata
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.380908</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">721</span>] <span class="hljs-string">updateVolumeSnapshotMetadata:</span> Metadata <span class="hljs-string">UID:</span> dc804d0d<span class="hljs-number">-139</span>a<span class="hljs-number">-11e9</span><span class="hljs-number">-9561</span><span class="hljs-number">-005056949728</span> Metadata <span class="hljs-string">Name:</span> xl-release-snapshot Metadata <span class="hljs-string">Namespace:</span> <span class="hljs-keyword">default</span> Setting tags <span class="hljs-keyword">in</span> Metadata <span class="hljs-string">Labels:</span> map[string]string{<span class="hljs-string">"SnapshotMetadata-Timestamp"</span>:<span class="hljs-string">"1546989146380869451"</span>, <span class="hljs-string">"SnapshotMetadata-PVName"</span>:<span class="hljs-string">"pvc-5f9bd5ec-1398-11e9-9561-005056949728"</span>}.
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.391791</span>      <span class="hljs-number">1</span> snapshot-controller.<span class="hljs-string">go:</span><span class="hljs-number">197</span>] [CONTROLLER] OnUpdate <span class="hljs-string">oldObj:</span> v1.VolumeSnapshotSpec{<span class="hljs-string">PersistentVolumeClaimName:</span><span class="hljs-string">"xlr-data-pvc"</span>, <span class="hljs-string">SnapshotDataName:</span><span class="hljs-string">""</span>}
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.391860</span>      <span class="hljs-number">1</span> snapshot-controller.<span class="hljs-string">go:</span><span class="hljs-number">198</span>] [CONTROLLER] OnUpdate <span class="hljs-string">newObj:</span> v1.VolumeSnapshotSpec{<span class="hljs-string">PersistentVolumeClaimName:</span><span class="hljs-string">"xlr-data-pvc"</span>, <span class="hljs-string">SnapshotDataName:</span><span class="hljs-string">""</span>}
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.392281</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">742</span>] <span class="hljs-string">updateVolumeSnapshotMetadata:</span> returning cloudTags [map[string]string{<span class="hljs-string">"kubernetes.io/created-for/snapshot/namespace"</span>:<span class="hljs-string">"default"</span>, <span class="hljs-string">"kubernetes.io/created-for/snapshot/name"</span>:<span class="hljs-string">"xl-release-snapshot"</span>, <span class="hljs-string">"kubernetes.io/created-for/snapshot/uid"</span>:<span class="hljs-string">"dc804d0d-139a-11e9-9561-005056949728"</span>, <span class="hljs-string">"kubernetes.io/created-for/snapshot/timestamp"</span>:<span class="hljs-string">"1546989146380869451"</span>}]
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.392661</span>      <span class="hljs-number">1</span> snapshot.<span class="hljs-string">go:</span><span class="hljs-number">53</span>] snapshot Spec <span class="hljs-string">Created:</span>
{<span class="hljs-string">"metadata"</span>:{<span class="hljs-string">"name"</span>:<span class="hljs-string">"pvc-5f9bd5ec-1398-11e9-9561-005056949728_xl-release-snapshot_1546989146392411824"</span>,<span class="hljs-string">"namespace"</span>:<span class="hljs-string">"default"</span>,<span class="hljs-string">"creationTimestamp"</span>:<span class="hljs-literal">null</span>},<span class="hljs-string">"spec"</span>:{<span class="hljs-string">"casType"</span>:<span class="hljs-string">"jiva"</span>,<span class="hljs-string">"volumeName"</span>:<span class="hljs-string">"pvc-5f9bd5ec-1398-11e9-9561-005056949728"</span>}}
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.596285</span>      <span class="hljs-number">1</span> snapshot.<span class="hljs-string">go:</span><span class="hljs-number">84</span>] Snapshot Successfully <span class="hljs-string">Created:</span>
{<span class="hljs-string">"apiVersion"</span>:<span class="hljs-string">"v1alpha1"</span>,<span class="hljs-string">"kind"</span>:<span class="hljs-string">"CASSnapshot"</span>,<span class="hljs-string">"metadata"</span>:{<span class="hljs-string">"name"</span>:<span class="hljs-string">"pvc-5f9bd5ec-1398-11e9-9561-005056949728_xl-release-snapshot_1546989146392411824"</span>},<span class="hljs-string">"spec"</span>:{<span class="hljs-string">"casType"</span>:<span class="hljs-string">"jiva"</span>,<span class="hljs-string">"volumeName"</span>:<span class="hljs-string">"pvc-5f9bd5ec-1398-11e9-9561-005056949728"</span>}}
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.596362</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">276</span>] snapshot <span class="hljs-string">created:</span> &amp;{&lt;nil&gt; &lt;nil&gt; &lt;nil&gt; &lt;nil&gt; &lt;nil&gt; <span class="hljs-number">0xc420038a00</span>}. <span class="hljs-string">Conditions:</span> &amp;[]v1.VolumeSnapshotCondition{v1.VolumeSnapshotCondition{<span class="hljs-string">Type:</span><span class="hljs-string">"Ready"</span>, <span class="hljs-string">Status:</span><span class="hljs-string">"True"</span>, <span class="hljs-string">LastTransitionTime:</span>v1.Time{<span class="hljs-string">Time:</span>time.Time{<span class="hljs-string">wall:</span><span class="hljs-number">0xbf056976a38b90b7</span>, <span class="hljs-string">ext:</span><span class="hljs-number">18032657942280</span>, <span class="hljs-string">loc:</span>(*time.Location)(<span class="hljs-number">0x2a17900</span>)}}, <span class="hljs-string">Reason:</span><span class="hljs-string">""</span>, <span class="hljs-string">Message:</span><span class="hljs-string">"Snapshot created successfully"</span>}}
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.596439</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">508</span>] <span class="hljs-string">createSnapshot:</span> create VolumeSnapshotData object <span class="hljs-keyword">for</span> VolumeSnapshot <span class="hljs-keyword">default</span>/xl-release-snapshot-dc804d0d<span class="hljs-number">-139</span>a<span class="hljs-number">-11e9</span><span class="hljs-number">-9561</span><span class="hljs-number">-005056949728.</span>
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.596478</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">533</span>] <span class="hljs-string">createVolumeSnapshotData:</span> Snapshot <span class="hljs-keyword">default</span>/xl-release-snapshot-dc804d0d<span class="hljs-number">-139</span>a<span class="hljs-number">-11e9</span><span class="hljs-number">-9561</span><span class="hljs-number">-005056949728.</span> <span class="hljs-string">Conditions:</span> &amp;[]v1.VolumeSnapshotCondition{v1.VolumeSnapshotCondition{<span class="hljs-string">Type:</span><span class="hljs-string">"Ready"</span>, <span class="hljs-string">Status:</span><span class="hljs-string">"True"</span>, <span class="hljs-string">LastTransitionTime:</span>v1.Time{<span class="hljs-string">Time:</span>time.Time{<span class="hljs-string">wall:</span><span class="hljs-number">0xbf056976a38b90b7</span>, <span class="hljs-string">ext:</span><span class="hljs-number">18032657942280</span>, <span class="hljs-string">loc:</span>(*time.Location)(<span class="hljs-number">0x2a17900</span>)}}, <span class="hljs-string">Reason:</span><span class="hljs-string">""</span>, <span class="hljs-string">Message:</span><span class="hljs-string">"Snapshot created successfully"</span>}}
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.604409</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">514</span>] <span class="hljs-string">createSnapshot:</span> Update VolumeSnapshot status and bind VolumeSnapshotData to VolumeSnapshot <span class="hljs-keyword">default</span>/xl-release-snapshot-dc804d0d<span class="hljs-number">-139</span>a<span class="hljs-number">-11e9</span><span class="hljs-number">-9561</span><span class="hljs-number">-005056949728.</span>
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.604456</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">860</span>] In bindVolumeSnapshotDataToVolumeSnapshot
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.604472</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">862</span>] <span class="hljs-string">bindVolumeSnapshotDataToVolumeSnapshot:</span> Namespace <span class="hljs-keyword">default</span> Name xl-release-snapshot
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.608792</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">877</span>] <span class="hljs-string">bindVolumeSnapshotDataToVolumeSnapshot:</span> Updating VolumeSnapshot object [&amp;v1.VolumeSnapshot{<span class="hljs-string">TypeMeta:</span>v1.TypeMeta{<span class="hljs-string">Kind:</span><span class="hljs-string">""</span>, <span class="hljs-string">APIVersion:</span><span class="hljs-string">""</span>}, <span class="hljs-string">Metadata:</span>v1.ObjectMeta{<span class="hljs-string">Name:</span><span class="hljs-string">"xl-release-snapshot"</span>, <span class="hljs-string">GenerateName:</span><span class="hljs-string">""</span>, <span class="hljs-string">Namespace:</span><span class="hljs-string">"default"</span>, <span class="hljs-string">SelfLink:</span><span class="hljs-string">"/apis/volumesnapshot.external-storage.k8s.io/v1/namespaces/default/volumesnapshots/xl-release-snapshot"</span>, <span class="hljs-string">UID:</span><span class="hljs-string">"dc804d0d-139a-11e9-9561-005056949728"</span>, <span class="hljs-string">ResourceVersion:</span><span class="hljs-string">"2072354"</span>, <span class="hljs-string">Generation:</span><span class="hljs-number">2</span>, <span class="hljs-string">CreationTimestamp:</span>v1.Time{<span class="hljs-string">Time:</span>time.Time{<span class="hljs-string">wall:</span><span class="hljs-number">0x0</span>, <span class="hljs-string">ext:</span><span class="hljs-number">63682585945</span>, <span class="hljs-string">loc:</span>(*time.Location)(<span class="hljs-number">0x2a17900</span>)}}, <span class="hljs-string">DeletionTimestamp:</span>(*v1.Time)(nil), <span class="hljs-string">DeletionGracePeriodSeconds:</span>(*int64)(nil), <span class="hljs-string">Labels:</span>map[string]string{<span class="hljs-string">"SnapshotMetadata-Timestamp"</span>:<span class="hljs-string">"1546989146380869451"</span>, <span class="hljs-string">"SnapshotMetadata-PVName"</span>:<span class="hljs-string">"pvc-5f9bd5ec-1398-11e9-9561-005056949728"</span>}, <span class="hljs-string">Annotations:</span>map[string]string{<span class="hljs-string">"kubectl.kubernetes.io/last-applied-configuration"</span>:<span class="hljs-string">"{\"apiVersion\":\"volumesnapshot.external-storage.k8s.io/v1\",\"kind\":\"VolumeSnapshot\",\"metadata\":{\"annotations\":{},\"name\":\"xl-release-snapshot\",\"namespace\":\"default\"},\"spec\":{\"persistentVolumeClaimName\":\"xlr-data-pvc\"}}\n"</span>}, <span class="hljs-string">OwnerReferences:</span>[]v1.OwnerReference(nil), <span class="hljs-string">Initializers:</span>(*v1.Initializers)(nil), <span class="hljs-string">Finalizers:</span>[]string(nil), <span class="hljs-string">ClusterName:</span><span class="hljs-string">""</span>}, <span class="hljs-string">Spec:</span>v1.VolumeSnapshotSpec{<span class="hljs-string">PersistentVolumeClaimName:</span><span class="hljs-string">"xlr-data-pvc"</span>, <span class="hljs-string">SnapshotDataName:</span><span class="hljs-string">"k8s-volume-snapshot-dd0c3a0d-139a-11e9-a875-467fb97678b7"</span>}, <span class="hljs-string">Status:</span>v1.VolumeSnapshotStatus{<span class="hljs-string">CreationTimestamp:</span>v1.Time{<span class="hljs-string">Time:</span>time.Time{<span class="hljs-string">wall:</span><span class="hljs-number">0x0</span>, <span class="hljs-string">ext:</span><span class="hljs-number">0</span>, <span class="hljs-string">loc:</span>(*time.Location)(nil)}}, <span class="hljs-string">Conditions:</span>[]v1.VolumeSnapshotCondition{v1.VolumeSnapshotCondition{<span class="hljs-string">Type:</span><span class="hljs-string">"Ready"</span>, <span class="hljs-string">Status:</span><span class="hljs-string">"True"</span>, <span class="hljs-string">LastTransitionTime:</span>v1.Time{<span class="hljs-string">Time:</span>time.Time{<span class="hljs-string">wall:</span><span class="hljs-number">0xbf056976a38b90b7</span>, <span class="hljs-string">ext:</span><span class="hljs-number">18032657942280</span>, <span class="hljs-string">loc:</span>(*time.Location)(<span class="hljs-number">0x2a17900</span>)}}, <span class="hljs-string">Reason:</span><span class="hljs-string">""</span>, <span class="hljs-string">Message:</span><span class="hljs-string">"Snapshot created successfully"</span>}}}}]
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.617060</span>      <span class="hljs-number">1</span> snapshot-controller.<span class="hljs-string">go:</span><span class="hljs-number">197</span>] [CONTROLLER] OnUpdate <span class="hljs-string">oldObj:</span> v1.VolumeSnapshotSpec{<span class="hljs-string">PersistentVolumeClaimName:</span><span class="hljs-string">"xlr-data-pvc"</span>, <span class="hljs-string">SnapshotDataName:</span><span class="hljs-string">""</span>}
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.617102</span>      <span class="hljs-number">1</span> snapshot-controller.<span class="hljs-string">go:</span><span class="hljs-number">198</span>] [CONTROLLER] OnUpdate <span class="hljs-string">newObj:</span> v1.VolumeSnapshotSpec{<span class="hljs-string">PersistentVolumeClaimName:</span><span class="hljs-string">"xlr-data-pvc"</span>, <span class="hljs-string">SnapshotDataName:</span><span class="hljs-string">"k8s-volume-snapshot-dd0c3a0d-139a-11e9-a875-467fb97678b7"</span>}
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.617118</span>      <span class="hljs-number">1</span> desired_state_of_world.<span class="hljs-string">go:</span><span class="hljs-number">76</span>] Adding <span class="hljs-keyword">new</span> snapshot to desired state of <span class="hljs-string">world:</span> <span class="hljs-keyword">default</span>/xl-release-snapshot-dc804d0d<span class="hljs-number">-139</span>a<span class="hljs-number">-11e9</span><span class="hljs-number">-9561</span><span class="hljs-number">-005056949728</span>
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.617449</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">202</span>] In <span class="hljs-string">waitForSnapshot:</span> snapshot <span class="hljs-keyword">default</span>/xl-release-snapshot-dc804d0d<span class="hljs-number">-139</span>a<span class="hljs-number">-11e9</span><span class="hljs-number">-9561</span><span class="hljs-number">-005056949728</span> snapshot data k8s-volume-snapshot-dd0c3a0d<span class="hljs-number">-139</span>a<span class="hljs-number">-11e9</span>-a875<span class="hljs-number">-467</span>fb97678b7
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.620951</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">241</span>] <span class="hljs-string">waitForSnapshot:</span> Snapshot <span class="hljs-keyword">default</span>/xl-release-snapshot-dc804d0d<span class="hljs-number">-139</span>a<span class="hljs-number">-11e9</span><span class="hljs-number">-9561</span><span class="hljs-number">-005056949728</span> created successfully. Adding it to Actual State of World.
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.620991</span>      <span class="hljs-number">1</span> actual_state_of_world.<span class="hljs-string">go:</span><span class="hljs-number">74</span>] Adding <span class="hljs-keyword">new</span> snapshot to actual state of <span class="hljs-string">world:</span> <span class="hljs-keyword">default</span>/xl-release-snapshot-dc804d0d<span class="hljs-number">-139</span>a<span class="hljs-number">-11e9</span><span class="hljs-number">-9561</span><span class="hljs-number">-005056949728</span>
I0108 <span class="hljs-number">23</span>:<span class="hljs-number">12</span>:<span class="hljs-number">26.621005</span>      <span class="hljs-number">1</span> snapshotter.<span class="hljs-string">go:</span><span class="hljs-number">526</span>] <span class="hljs-string">createSnapshot:</span> Snapshot <span class="hljs-keyword">default</span>/xl-release-snapshot-dc804d0d<span class="hljs-number">-139</span>a<span class="hljs-number">-11e9</span><span class="hljs-number">-9561</span><span class="hljs-number">-005056949728</span> created successfully.
I0109 <span class="hljs-number">00</span>:<span class="hljs-number">11</span>:<span class="hljs-number">54.211526</span>      <span class="hljs-number">1</span> snapshot-controller.<span class="hljs-string">go:</span><span class="hljs-number">197</span>] [CONTROLLER] OnUpdate <span class="hljs-string">oldObj:</span> v1.VolumeSnapshotSpec{<span class="hljs-string">PersistentVolumeClaimName:</span><span class="hljs-string">"xlr-data-pvc"</span>, <span class="hljs-string">SnapshotDataName:</span><span class="hljs-string">"k8s-volume-snapshot-dd0c3a0d-139a-11e9-a875-467fb97678b7"</span>}
I0109 <span class="hljs-number">00</span>:<span class="hljs-number">11</span>:<span class="hljs-number">54.211695</span>      <span class="hljs-number">1</span> snapshot-controller.<span class="hljs-string">go:</span><span class="hljs-number">198</span>] [CONTROLLER] OnUpdate <span class="hljs-string">newObj:</span> v1.VolumeSnapshotSpec{<span class="hljs-string">PersistentVolumeClaimName:</span><span class="hljs-string">"xlr-data-pvc"</span>, <span class="hljs-string">SnapshotDataName:</span><span class="hljs-string">"k8s-volume-snapshot-dd0c3a0d-139a-11e9-a875-467fb97678b7"</span>}
I0109 <span class="hljs-number">01</span>:<span class="hljs-number">11</span>:<span class="hljs-number">54.211693</span>      <span class="hljs-number">1</span> snapshot-controller.<span class="hljs-string">go:</span><span class="hljs-number">197</span>] [CONTROLLER] OnUpdate <span class="hljs-string">oldObj:</span> v1.VolumeSnapshotSpec{<span class="hljs-string">PersistentVolumeClaimName:</span><span class="hljs-string">"xlr-data-pvc"</span>, <span class="hljs-string">SnapshotDataName:</span><span class="hljs-string">"k8s-volume-snapshot-dd0c3a0d-139a-11e9-a875-467fb97678b7"</span>}
I0109 <span class="hljs-number">01</span>:<span class="hljs-number">11</span>:<span class="hljs-number">54.211817</span>      <span class="hljs-number">1</span> snapshot-controller.<span class="hljs-string">go:</span><span class="hljs-number">198</span>] [CONTROLLER] OnUpdate <span class="hljs-string">newObj:</span> v1.VolumeSnapshotSpec{<span class="hljs-string">PersistentVolumeClaimName:</span><span class="hljs-string">"xlr-data-pvc"</span>, <span class="hljs-string">SnapshotDataName:</span><span class="hljs-string">"k8s-volume-snapshot-dd0c3a0d-139a-11e9-a875-467fb97678b7"</span>}
I0109 <span class="hljs-number">02</span>:<span class="hljs-number">11</span>:<span class="hljs-number">54.211890</span>      <span class="hljs-number">1</span> snapshot-controller.<span class="hljs-string">go:</span><span class="hljs-number">197</span>] [CONTROLLER] OnUpdate <span class="hljs-string">oldObj:</span> v1.VolumeSnapshotSpec{<span class="hljs-string">PersistentVolumeClaimName:</span><span class="hljs-string">"xlr-data-pvc"</span>, <span class="hljs-string">SnapshotDataName:</span><span class="hljs-string">"k8s-volume-snapshot-dd0c3a0d-139a-11e9-a875-467fb97678b7"</span>}
I0109 <span class="hljs-number">02</span>:<span class="hljs-number">11</span>:<span class="hljs-number">54.212010</span>      <span class="hljs-number">1</span> snapshot-controller.<span class="hljs-string">go:</span><span class="hljs-number">198</span>] [CONTROLLER] OnUpdate <span class="hljs-string">newObj:</span> v1.VolumeSnapshotSpec{<span class="hljs-string">PersistentVolumeClaimName:</span><span class="hljs-string">"xlr-data-pvc"</span>, <span class="hljs-string">SnapshotDataName:</span><span class="hljs-string">"k8s-volume-snapshot-dd0c3a0d-139a-11e9-a875-467fb97678b7"</span>}
I0109 <span class="hljs-number">03</span>:<span class="hljs-number">11</span>:<span class="hljs-number">54.212062</span>      <span class="hljs-number">1</span> snapshot-controller.<span class="hljs-string">go:</span><span class="hljs-number">197</span>] [CONTROLLER] OnUpdate <span class="hljs-string">oldObj:</span> v1.VolumeSnapshotSpec{<span class="hljs-string">PersistentVolumeClaimName:</span><span class="hljs-string">"xlr-data-pvc"</span>, <span class="hljs-string">SnapshotDataName:</span><span class="hljs-string">"k8s-volume-snapshot-dd0c3a0d-139a-11e9-a875-467fb97678b7"</span>}
I0109 <span class="hljs-number">03</span>:<span class="hljs-number">11</span>:<span class="hljs-number">54.212201</span>      <span class="hljs-number">1</span> snapshot-controller.<span class="hljs-string">go:</span><span class="hljs-number">198</span>] [CONTROLLER] OnUpdate <span class="hljs-string">newObj:</span> v1.VolumeSnapshotSpec{<span class="hljs-string">PersistentVolumeClaimName:</span><span class="hljs-string">"xlr-data-pvc"</span>, <span class="hljs-string">SnapshotDataName:</span><span class="hljs-string">"k8s-volume-snapshot-dd0c3a0d-139a-11e9-a875-467fb97678b7"</span>}
I0109 <span class="hljs-number">04</span>:<span class="hljs-number">11</span>:<span class="hljs-number">54.212249</span>      <span class="hljs-number">1</span> snapshot-controller.<span class="hljs-string">go:</span><span class="hljs-number">197</span>] [CONTROLLER] OnUpdate <span class="hljs-string">oldObj:</span> v1.VolumeSnapshotSpec{<span class="hljs-string">PersistentVolumeClaimName:</span><span class="hljs-string">"xlr-data-pvc"</span>,
</code></pre>
<p><strong>Resolution:</strong></p>
<p>This can be happen due to the stale entries of snapshot and snapshot data. By deleting those entries will resolve this issue.</p>
<p><br></p>
<hr>
<br>
<p><font size="6" color="green">Kubernetes related</font></p>
<p><br></p>
<p><h3><a class="anchor" aria-hidden="true" id="node-reboot-when-kubelet-memory-increases"></a>Kubernetes node reboots because of increase in memory consumed by Kubelet</h3>
Sometime it is observed that iscsiadm is  continuously fails and repeats rapidly and for some reason this causes the memory consumption of kubelet to grow until the node goes out-of-memory and needs to be rebooted. Following type of error can be observed in journalctl and cstor-istgt container.</p>
<p><strong>journalctl logs</strong></p>
<pre><code class="hljs">Feb 06 06:11:38 &lt;hostname&gt; kubelet[1063]: iscsiadm: failed <span class="hljs-keyword">to</span> send SendTargets PDU
Feb 06 06:11:38 &lt;hostname&gt; kubelet[1063]: iscsiadm:<span class="hljs-built_in"> connection </span>login retries (reopen_max) 5 exceeded
Feb 06 06:11:38 &lt;hostname&gt; kubelet[1063]: iscsiadm:<span class="hljs-built_in"> Connection </span><span class="hljs-keyword">to</span><span class="hljs-built_in"> Discovery Address </span>10.233.46.76 failed
Feb 06 06:11:38 &lt;hostname&gt; kubelet[1063]: iscsiadm: failed <span class="hljs-keyword">to</span> send SendTargets PDU
Feb 06 06:11:38 &lt;hostname&gt; kubelet[1063]: iscsiadm:<span class="hljs-built_in"> connection </span>login retries (reopen_max) 5 exceeded
Feb 06 06:11:38 &lt;hostname&gt; kubelet[1063]: iscsiadm:<span class="hljs-built_in"> Connection </span><span class="hljs-keyword">to</span><span class="hljs-built_in"> Discovery Address </span>10.233.46.76 failed
Feb 06 06:11:38 &lt;hostname&gt; kubelet[1063]: iscsiadm: failed <span class="hljs-keyword">to</span> send SendTargets PDU
Feb 06 06:11:38 &lt;hostname&gt; kubelet[1063]: iscsiadm:<span class="hljs-built_in"> connection </span>login retries (reopen_max) 5 exceeded
Feb 06 06:11:38 &lt;hostname&gt; kubelet[1063]: iscsiadm:<span class="hljs-built_in"> Connection </span><span class="hljs-keyword">to</span><span class="hljs-built_in"> Discovery Address </span>10.233.46.76 failed
Feb 06 06:11:38 &lt;hostname&gt; kubelet[1063]: iscsiadm: failed <span class="hljs-keyword">to</span> send SendTargets PDU
</code></pre>
<p><strong>cstor-istgt container logs</strong></p>
<pre><code class="hljs"><span class="hljs-number">2019-02-05</span>/<span class="hljs-number">15:43:30.250</span> worker            :<span class="hljs-number">6088</span>: c#<span class="hljs-number">0</span>.<span class="hljs-number">140005771040512</span>.: iscsi_read_pdu() EOF

<span class="hljs-number">2019-02-05</span>/<span class="hljs-number">15:43:30.250</span> sender            :<span class="hljs-number">5852</span>: s#<span class="hljs-number">0</span>.<span class="hljs-number">140005666154240</span>.: sender loop ended (<span class="hljs-number">0:14:43084</span>)

<span class="hljs-number">2019-02-05</span>/<span class="hljs-number">15:43:30.251</span> worker            :<span class="hljs-number">6292</span>: c#<span class="hljs-number">0</span>.<span class="hljs-number">140005771040512</span>.: worker <span class="hljs-number">0</span>/-<span class="hljs-number">1/43084</span> end (c#<span class="hljs-number">0</span>.<span class="hljs-number">140005771040512</span>/s#<span class="hljs-number">0</span>.<span class="hljs-number">140005666154240</span>)
<span class="hljs-number">2019-02-05</span>/<span class="hljs-number">15</span>:<span class="hljs-number">43:30.264</span> worker            :<span class="hljs-number">5885</span>: c#<span class="hljs-number">1</span>.<span class="hljs-number">140005666154240</span>.: con:<span class="hljs-number">1</span>/<span class="hljs-number">16</span> [<span class="hljs-number">8d614b93</span>:<span class="hljs-number">43088</span>-&gt;<span class="hljs-number">10.233.45.100</span>:<span class="hljs-number">3260</span>,<span class="hljs-number">1</span>]
<span class="hljs-number">2019-02-05</span>/<span class="hljs-number">15</span>:<span class="hljs-number">43:30.531</span> istgt_iscsi_op_log:<span class="hljs-number">1923</span>: c#<span class="hljs-number">1</span>.<span class="hljs-number">140005666154240</span>.: login failed, target not ready

<span class="hljs-number">2019-02-05</span>/<span class="hljs-number">15</span>:<span class="hljs-number">43:30.782</span> worker            :<span class="hljs-number">6088</span>: c#<span class="hljs-number">1</span>.<span class="hljs-number">140005666154240</span>.: iscsi_read_pdu() EOF

<span class="hljs-number">2019-02-05</span>/<span class="hljs-number">15</span>:<span class="hljs-number">43:30.782</span> sender            :<span class="hljs-number">5852</span>: s#<span class="hljs-number">1</span>.<span class="hljs-number">140005649413888</span>.: sender loop ended (<span class="hljs-number">1:16:43088</span>)

<span class="hljs-number">2019-02-05</span>/<span class="hljs-number">15</span>:<span class="hljs-number">43:30.783</span> worker            :<span class="hljs-number">6292</span>: c#<span class="hljs-number">1</span>.<span class="hljs-number">140005666154240</span>.: worker <span class="hljs-number">1</span>/-<span class="hljs-number">1/43088</span> end (c#<span class="hljs-number">1</span>.<span class="hljs-number">140005666154240</span>/s#<span class="hljs-number">1</span>.<span class="hljs-number">140005649413888</span>)
<span class="hljs-number">2019-02-05</span>/<span class="hljs-number">15</span>:<span class="hljs-number">43:33.285</span> worker            :<span class="hljs-number">5885</span>: c#<span class="hljs-number">2</span>.<span class="hljs-number">140005649413888</span>.: con:<span class="hljs-number">2</span>/<span class="hljs-number">18</span> [<span class="hljs-number">8d614b93</span>:<span class="hljs-number">43092</span>-&gt;<span class="hljs-number">10.233.45.100</span>:<span class="hljs-number">3260</span>,<span class="hljs-number">1</span>]
<span class="hljs-number">2019-02-05</span>/<span class="hljs-number">15</span>:<span class="hljs-number">43:33.536</span> istgt_iscsi_op_log:<span class="hljs-number">1923</span>: c#<span class="hljs-number">2</span>.<span class="hljs-number">140005649413888</span>.: login failed, target not ready

<span class="hljs-number">2019-02-05</span>/<span class="hljs-number">15</span>:<span class="hljs-number">43:33.787</span> worker            :<span class="hljs-number">6088</span>: c#<span class="hljs-number">2</span>.<span class="hljs-number">140005649413888</span>.: iscsi_read_pdu() EOF

<span class="hljs-number">2019-02-05</span>/<span class="hljs-number">15</span>:<span class="hljs-number">43:33.787</span> sender            :<span class="hljs-number">5852</span>: s#<span class="hljs-number">2</span>.<span class="hljs-number">140005632636672</span>.: sender loop ended (<span class="hljs-number">2:18:43092</span>)

<span class="hljs-number">2019-02-05</span>/<span class="hljs-number">15</span>:<span class="hljs-number">43:33.788</span> worker            :<span class="hljs-number">6292</span>: c#<span class="hljs-number">2</span>.<span class="hljs-number">140005649413888</span>.: worker <span class="hljs-number">2</span>/-<span class="hljs-number">1/43092</span> end (c#<span class="hljs-number">2</span>.<span class="hljs-number">140005649413888</span>/s#<span class="hljs-number">2</span>.<span class="hljs-number">140005632636672</span>)
<span class="hljs-number">2019-02-05</span>/<span class="hljs-number">15:43:35.251</span> istgt_remove_conn :<span class="hljs-number">7039</span>: c#<span class="hljs-number">0</span>.<span class="hljs-number">140005771040512</span>.: remove_conn-&gt;initiator:<span class="hljs-number">147.75.97.141</span>(iqn.<span class="hljs-number">2019</span>-<span class="hljs-number">02</span>.net.packet:device.<span class="hljs-number">7</span>c8ad781) Target: <span class="hljs-number">10.233.109.82</span>(dummy LU0) conn:<span class="hljs-number">0</span>x7f<span class="hljs-number">55a4c18000</span>:<span class="hljs-number">0</span> tsih:<span class="hljs-number">1</span> connections:<span class="hljs-number">0</span>  IOPending=<span class="hljs-number">0</span>
<span class="hljs-number">2019-02-05</span>/<span class="hljs-number">15</span>:<span class="hljs-number">43:36.291</span> worker            :<span class="hljs-number">5885</span>: c#<span class="hljs-number">0</span>.<span class="hljs-number">140005666154240</span>.: con:<span class="hljs-number">0</span>/<span class="hljs-number">14</span> [<span class="hljs-number">8d614b93</span>:<span class="hljs-number">43094</span>-&gt;<span class="hljs-number">10.233.45.100</span>:<span class="hljs-number">3260</span>,<span class="hljs-number">1</span>]
<span class="hljs-number">2019-02-05</span>/<span class="hljs-number">15</span>:<span class="hljs-number">43:36.540</span> istgt_iscsi_op_log:<span class="hljs-number">1923</span>: c#<span class="hljs-number">0</span>.<span class="hljs-number">140005666154240</span>.: login failed, target not ready
</code></pre>
<p><strong>Troubleshooting</strong></p>
<p>The cause of high memory consumption of kubelet is mainly due to the following.</p>
<p>There are 3 modules are involved - cstor-isgt, kubelet and iscsiInitiator(iscsiadm).
kubelet runs iscsiadm command to do discovery on cstor-istgt.
If there is any delay in receiving response of discovery opcode (either due to network or delay in processing on target side), iscsiadm retries few times, and, gets into infinite loop dumping error messages as below:</p>
<pre><code class="hljs">iscsiadm: Connection to Discovery Address 127.0.0.1 failed
iscsiadm: failed to send SendTargets PDU
iscsiadm: connection login retries (reopen_max) 5 exceeded
iscsiadm: Connection to Discovery Address 127.0.0.1 failed
iscsiadm: failed to send SendTargets PDU```
kubelet keeps taking this response and accumulates the memory.
</code></pre>
<p><strong>Workaround</strong></p>
<p>Restart the corresponding istgt pod to avoid memory consumption.</p>
<p><h3><a class="anchor" aria-hidden="true" id="Pods-restart-terminate-when-heavy-load"></a>Application and OpenEBS pods terminate/restart under heavy I/O load</h3></p>
<p>This is caused due to lack of resources on the Kubernetes nodes, which causes the pods to evict under loaded conditions as the node becomes <em>unresponsive</em>. The pods transition from <em>Running</em> state to <em>unknown</em> state followed by <em>Terminating</em> before restarting again.</p>
<p><strong>Troubleshooting</strong></p>
<p>The above cause can be confirmed from the <code>kubectl describe pod</code> which displays the termination reason as <em>NodeControllerEviction</em>. You can get more information from the kube-controller-manager.log on the Kubernetes master.</p>
<p><strong>Workaround:</strong></p>
<p>You can resolve this issue by upgrading the Kubernetes cluster infrastructure resources (Memory, CPU).</p>
<p><br></p>
<hr>
<p><br><br></p>
<h2><a class="anchor" aria-hidden="true" id="see-also"></a><a href="#see-also" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>See Also:</h2>
<h3><a class="anchor" aria-hidden="true" id="faqs-docs-next-faqhtml"></a><a href="#faqs-docs-next-faqhtml" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><a href="/docs/next/faq.html">FAQs</a></h3>
<h3><a class="anchor" aria-hidden="true" id="seek-support-or-help-docs-next-supporthtml"></a><a href="#seek-support-or-help-docs-next-supporthtml" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><a href="/docs/next/support.html">Seek support or help</a></h3>
<h3><a class="anchor" aria-hidden="true" id="latest-release-notes"></a><a href="#latest-release-notes" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><a href="">Latest release notes</a></h3>
<p><br></p>
<hr>
<p><br></p>
<!-- Hotjar Tracking Code for https://docs.openebs.io -->
<script>
   (function(h,o,t,j,a,r){
       h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
       h._hjSettings={hjid:785693,hjsv:6};
       a=o.getElementsByTagName('head')[0];
       r=o.createElement('script');r.async=1;
       r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
       a.appendChild(r);
   })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-92076314-12"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
<p>gtag('config', 'UA-92076314-12');</p>
</script>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="k8supgrades.html">← Best practices to follow when upgrading Kubernetes</a></div></div></div><nav class="onPageNav"><div id="global-on-page-heading"><h4 style="font-weight:400;">On this page:</h4><ul class="toc-headings"><li><a href="#installation">Installation</a></li><li><a href="#uninstall">Uninstall</a></li><li><a href="#volume-provisioning">Volume provisioning</a></li><li><a href="#kubernetes-related">Kubernetes related</a></li><li><a href="#see-also">See Also:</a><ul class="toc-headings"><li><a href="#faqs-docs-next-faqhtml">FAQs</a></li><li><a href="#seek-support-or-help-docs-next-supporthtml">Seek support or help</a></li><li><a href="#latest-release-notes">Latest release notes</a></li></ul></li></ul></div></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><div><br/><div><a href="https://openebs.io" target="_blank"><img src="/docs/assets/openebs-logo.svg" alt="" width="130" height="110"/></a></div><div><a href="https://slack.openebs.io">Get in touch with OpenEBS community via Slack</a></div></div><div><br/><div><a href="https://mayadata.io" target="_blank"><img src="https://openebs.io/assets/images/mayadata.svg" alt="mayadata.io" width="130" height="110"/></a></div><div><a href="https://app.mayaonline.io">Get OpenEBS support through MayaOnline</a></div></div></section><section class="copyright"></section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
              var search = docsearch({
                
                apiKey: 'dc657dfe30f42f228671f557f49ced7a',
                indexName: 'openebs',
                inputSelector: '#search_input_react'
              });
            </script><script>(function(){ window.ldfdr = window.ldfdr || {}; (function(d, s, ss, fs){ fs = d.getElementsByTagName(s)[0]; function ce(src){ var cs = d.createElement(s); cs.async=1; cs.src = src; setTimeout(function(){fs.parentNode.insertBefore(cs,fs)}, 1); } ce(ss); })(document, 'script', 'https://lftracker.leadfeeder.com/lftracker_v1_bMwm7E2pzw4gOZyA.js'); })();</script></body></html>